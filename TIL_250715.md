### 다중 분류 딥러닝
### 데이터 Tensor로 형 변환
- numpy 형식
  - ex) iris 데이터를 가져올 때
  ``` python
  from sklearn.datasets import load_iris
  import torch
  iris=load_iris()
  # 넘파이 형식 출력
  X = iris.data
  y = iris.target
  # 텐서로 형 변환하여 소수형태로 출력
  X = torch.from_numpy(X).float()
  y = torch.from_numpy(y).float()
  ```
- DataFrame 형식
  ``` python
  import panas as pd
  import torch
  df = pd.DataFrame(iris.data, columns=iris.feature_names)
  df['classs'] = iris.target
  X = df.drop('class',axis=1)
  y = df['class']
  ## 텐서로 형 변환하여 소수형태로 출력
  X = torch.tensor(X.values).float()
  ## 텐서로 형 변환하여 정수형태로 출력
  # 다중 분류 라벨값이라서 정수형으로 변환
  y = torch.tensor(y.values).long()
  ```

  # 데이터 분할(훈련, 테스트)
  ``` python
  from sklearn.model_selection import train_test_split

  X_trian,X_test, y_train,y_test = train_test_split(X,y,test_size=0.2, random_state=5)
  ```


  # 모델 클래스 선언
  # torch.nn.Module 사용
  ``` python
  import torch.nn as nn
  class MyModel(nn.Module):
    def __init__(self.input_dim, output_dim):
        # 부모클래스 상속
        super().__init__()
        self.linear = nn.Linear(input_dim,output_dim)
        # 다중 클래스 분류 모델 활성화 함수 - LogSoftmax() 사용
        self.act = nn. LogSoftmax(dim=1)
    # 예측함수
    def forward(self,x):
        return self.linear(X)
```
### 최적화 함수 생성
``` python
import torch.optim as optim
learning_rate = 0.001
# 모델 생성
# input_dim은 X_train의 피처의 수
# output_dim은 라벨 클래스의 고유값의 개수
model = MyModel(input_dim = X_train.size(-1),output_dim = len(y.unique()))

# 손실함수 설정
criterion = nn.CrossEntropyLoss()

# 최적화 함수 - 경사하강법
optimizer = optim.SGD(model.parameters(),lr= learning_rate)
```
### 모델 학습
``` python
from tqdm import tqdm  # 진행 상태(progress bar)를 표시하는 라이브러리
import numpy as np     # (현재 코드에서는 안 쓰이지만) 수치 계산을 위한 라이브러리
print_interval = 100
n_epochs = 1000  # 학습 반복 횟수(epoch) 설정

# tqdm으로 학습 진행 상황 표시
for i in tqdm(range(n_epochs)):
    model.train()  
    # 모델을 학습 모드로 설정

    pred_y = model(X_train)
    # 모델에 학습 데이터를 넣어서 예측값(pred_y) 생성

    loss = criterion(pred_y, y_train)
    # 예측값과 실제 정답(y_train)을 비교해 손실(loss) 계산

    optimizer.zero_grad()
    # 이전 epoch에서 계산된 gradient를 초기화

    loss.backward()
    # 현재 손실에 대한 gradient(기울기)를 역전파로 계산
    # 파라미터(w,b) 구함

    optimizer.step()
    # 계산된 gradient를 이용해 모델의 파라미터를 한 스텝 업데이트
    ## 파라미터(w,b) 적용
    if (i+1) % print_interval == 0 :
        # tensor인  loss를 소수점 처리하기 위해 .item() 사용!
        print(f"epoch{i+1} : loss {loss.item():.4f}")
        ```
