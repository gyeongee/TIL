### 영상 분류
- 영상을 입력으로 받아 영상 전체에 레이블 또는 어느 클래스에 속하는지 예측 반환
  
#### UCF101 데이터 세트의 하위 집합을 통해 VideoMAE 미세 조정하기
- UCF101 데이터 세트
    - 총 클래스 수: 101개 (human actions)
    - 총 비디오 수: 약 13,320개
    - 클래스별 비디오 수: 100 ~ 250개 사이
    - 비디오 길이: 평균 7초 내외
    - 해상도: 다양한 해상도 (최소 240×320 ~ 최대 HD 수준)
    - 파일 포맷: 대부분 .avi
    - 주요 클래스 예시
        Sports: Basketball, SoccerJuggling, TennisSwing
        Daily Actions: ApplyEyeMakeup, BrushingTeeth, Punch
        Musical Instruments: PlayingGuitar, PlayingPiano

### "UCF101_subset.tar.gz" 아카이브 파일 압축 풀기
```
import tarfile

with tarfile.open(file_path) as tar:
    tar.extractall(".") # 현재 폴더(.)에 모든 파일 추출
```

### 파일 목록 리스트에 저장
```
import os

# os.getcwd(): 현재폴더
current_path = os.path.join(os.getcwd(), "UCF101_subset") # 현재 작업 디렉토리(os.getcwd())와 "UCF101_subset" 폴더 이름을 연결

file_path=[]

# os.walk(): 모든 폴더를 순회하면서 파일을 가짐
for root,_,files in os.walk(current_path):
    for name in files:
        file_path.append(os.path.join(root,name))
file_path[50]

# ./UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c06.avi
```

### label2id: 클래스 이름을 정수에 매핑
### id2label: 정수를 클래스 이름에 매핑

```
class_labels = sorted(set(os.path.basename(path).split("_")[1] for path in file_path))
# ['ApplyEyeMakeup',
 'ApplyLipstick',
 'Archery',
 'BabyCrawling',
 'BalanceBeam',
 'BandMarching',
 'BaseballPitch',
 'Basketball',
 'BasketballDunk',
 'BenchPress']
 ```

 ```
label2id = {label : i for i, label in enumerate(class_labels)}
id2label = {i : label for label, i in label2id.items()}
```
### 전처리기 모델 가져오기
```
image_processor = VideoMAEImageProcessor.from_pretrained(model_id)
model = VideoMAEForVideoClassification.from_pretrained(model_id,
                                                       label2id=label2id,
                                                       id2label=id2label,
                                                       # 모델과 라벨수가 다르더라도 로딩하도록 설정
                                                       ignore_mismatched_sizes=True)
```

- 영상 프레임 픽셀을 정규화하는 데 사용되는 이미지 평균과 표준 편차와 영상 프레임이 조정될 공간 해상도 계산

```
# 평균과 표준편차 계산(정규화)
mean = image_processor.image_mean
std = image_processor.image_std

# 크기 패딩
# image_processor.size에 "shortest_edge" 키가 있으면 짧은 변 기준으로 크기 조정
if "shortest_edge" in image_processor.size:
    height = width = image_processpr.size["shortest_edge"]  # 세로와 가로를 동일하게 설정 (정사각형 기준)
else : # "shortest_edge"가 없으면 가로(height)와 세로(width) 크기 각각 설정
    height = image_processor.size["height"]
    width = image_processor.size["width"]
# 최종 resize할 크기 (튜플 형태: (height, width))
resize_to=(height,width)

# 영상 프레임 샘플링 관련 설정
num_frames_to_sample = model.config.num_frames  # 모델 입력에 필요한 총 프레임 수
sample_rate = 4                                  # 샘플링 속도 (몇 프레임마다 1개씩 추출할지)
fps = 30                                         # 원본 영상 프레임 속도 (frames per second)

# 총 플레이 시간 계산 (초 단위)
# clip_duration = (샘플링된 프레임 수 * 샘플링 간격) / 영상 FPS
clip_duration = num_frames_to_sample * sample_rate / fps
```

### 훈련 데이터 전처리
```
# 관련 라이브러리 설치
!pip uninstall pytorchvideo -y
!pip install -q git+https://github.com/facebookresearch/pytorchvideo.git
import pytorchvideo.data

# 정규화
from pytorchvideo.transforms import (
    ApplyTransformToKey,      # dict 형태로 되어 있는 데이터에서 특정 key에만 transform을 적용
    Normalize,                # 픽셀 값을 정규화 (mean/std 사용)
    RandomShortSideScale,     # 영상의 짧은 변(short side)을 랜덤한 크기로 리사이즈
    RemoveKey,                # dict에서 특정 key 제거
    ShortSideScale,           # 영상의 짧은 변(short side)을 고정된 크기로 리사이즈
    UniformTemporalSubsample  # 영상 프레임을 균등하게 샘플링
)

# 이미지 증식
from torchvision.transforms import (
    Compose,                  # 여러 transform을 순차적으로 적용
    Lambda,                   # 임의의 사용자 함수 적용
    RandomCrop,               # 이미지/영상에서 랜덤한 위치를 중심으로 crop
    RandomHorizontalFlip,     # 확률적으로 좌우 반전
    Resize                    # 이미지/영상 리사이즈
)

# 이미지 증식
train_transform = Compose(
    [
        # video 키의 값만 변환에 사용
        ApplyTransformToKey(
            key="video",
            transform=Compose(
                [
                    # 비디오 프레임을 num_frames_to_sample개 선택
                    UniformTemporalSubsample(num_frames_to_sample),
                    # 픽셀값을 0-1로 정규화
                    Lambda(lambda x: x / 255.0),
                    # 채널별 평균과 표준편차로 정규화
                    Normalize(mean, std),
                    # 비디오 짧은 길이를 기준으로 min_size~max_size 사이로 무작위 크기 변환
                    RandomShortSideScale(min_size=256, max_size=320),
                    # 스케일된 영상에서 resize_to 크기로 랜덤하게 자름
                    RandomCrop(resize_to),
                    # 50% 확률로 좌우반전
                    RandomHorizontalFlip(p=0.5)
                ]
            )
        )
    ]
)

train_dataset = pytorchvideo.data.Ucf101(
    # 훈련데이터 경로
    data_path=os.path.join(current_path, "train"),
    # 비디오 클립 샘플링 방법 (랜덤, 클립의 길이)
    # 만약 1-10프레임이 있을때 5프레임을 샘플링할때
    # 랜덤으로 1, 5, 6, 9, 10 -> 앞의 프레임만 순서대로 자르는 것(1,2,3,4,5)보다 비디오 정보를 더 잘 포함
    clip_sampler=pytorchvideo.data.make_clip_sampler("random", clip_duration),
    # 오디오 로딩 여부
    decode_audio=False,
    transform=train_transform
)
```

### 테스트, 검증 데이터 전처리
- 증식을 수행하지 않음 - 평가와 추론을 가짜 데이터로 하면 안 됨
  
```
# -------------------------------
# 검증/테스트 데이터용 변환(전처리) 정의
# -------------------------------
val_transform = Compose(
    [
        # "video" 키에만 아래 변환(transform) 적용
        ApplyTransformToKey(
            key="video",
            transform=Compose(
                [
                    # 영상에서 num_frames_to_sample 개수만큼 프레임을 균등하게 샘플링
                    UniformTemporalSubsample(num_frames_to_sample),
                    # 프레임 픽셀 값을 0~1 범위로 정규화
                    Lambda(lambda x: x / 255.0),
                    # 평균(mean)과 표준편차(std)를 이용해 정규화
                    Normalize(mean, std),
                    # 지정한 크기(resize_to)로 영상 크기 조정
                    Resize(resize_to)
                ]
            )
        )
    ]
)

# -------------------------------
# UCF101 검증 데이터셋 정의
# -------------------------------
val_dataset = pytorchvideo.data.Ucf101(
    # 검증 데이터 경로
    data_path=os.path.join(current_path, "val"),
    # 영상에서 균등하게 clip_duration 길이의 클립 샘플링
    clip_sampler=pytorchvideo.data.make_clip_sampler("uniform", clip_duration),
    # 오디오는 사용하지 않음
    decode_audio=False,
    # 영상 전처리(transform) 적용
    transform=val_transform
)

# -------------------------------
# UCF101 테스트 데이터셋 정의
# -------------------------------
test_dataset = pytorchvideo.data.Ucf101(
    # 테스트 데이터 경로
    data_path=os.path.join(current_path, "test"),
    # 영상에서 균등하게 clip_duration 길이의 클립 샘플링
    clip_sampler=pytorchvideo.data.make_clip_sampler("uniform", clip_duration),
    # 오디오는 사용하지 않음
    decode_audio=False,
    # 영상 전처리(transform) 적용
    transform=val_transform
)
```
-  데이터의 구조 변환, 텐서로 변환
```
def collate_fn(examples):
    # 데이터 구조 변환
    # (T, C, H, W)로 바꿉니다.
    # 0 (채널(C)), 1(프레임수(T)), 2(높이(H)), 2(너비(W))
    # (0, 1, 2, 3) -> (1, 0, 2, 3)
    # torch.stack() : 텐서들을 새로운 차원으로 쌓아서 하나의 텐서로 만들어 줍니다
    pixel_values = torch.stack(
        [example["video].permute(1,0,2,3) for example in examples]
    )

    # 라벨을 텐서로 변환
    labels = torch.tensor([example["label"] for example in examples])
    return {"pixel_values":pixel_values, "labels":labels}
    ```