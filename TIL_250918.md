### 프로젝트 설명
- 1. 이상행동(범죄) 관련 영상을 업로드 후 영상 라벨링 결과를 화면에 출력
- 2. 화면에 올린 영상 재생
- 3. 실종 데이터(csv 파일) 기반 RAG를 django를 이용하여 화면에 질문과 답변 출력

##### 1-1. onnx 파일 생성
- 내가 커스텀한 Pyrorch모델은 영상 분류 모델이라 huggingface에 업로드를 해도 API키를 부여할 수 없었음
- 모델을 onnx파일로 변환 후 모델 사용
  ```
import torch
from transformers import VideoMAEForVideoClassification, VideoMAEConfig
from safetensors.torch import load_file

# -------------------------------
# 경로 설정
model_dir = "./dmsrud/anomalous_behavior_video_cls_model"
safetensor_path = f"{model_dir}/model.safetensors"
config_path = f"{model_dir}/config.json"
onnx_path = f"{model_dir}/video_cls_model.onnx"

# -------------------------------
# config 불러오기
config = VideoMAEConfig.from_json_file(config_path)

# -------------------------------
# 모델 생성
model = VideoMAEForVideoClassification(config)
# safetensors 로드
weights = load_file(safetensor_path)
model.load_state_dict(weights)
model.eval()  # ONNX export 전 반드시 eval 모드

# -------------------------------
# dummy input 생성
B = 1  # batch size
T = config.num_frames  # 16
C = config.num_channels  # 3
H = W = config.image_size  # 224

dummy_input = torch.randn(B, T, C, H, W, dtype=torch.float32)

# -------------------------------
# ONNX 변환
torch.onnx.export(
    model,
    dummy_input,
    onnx_path,
    input_names=["pixel_values"],
    output_names=["logits"],
    opset_version=17,
    dynamic_axes={
        "pixel_values": {0: "batch_size", 1: "num_frames"},
        "logits": {0: "batch_size"}
    }
)

print(f"ONNX 모델 저장 완료: {onnx_path}")
```
##### 1-2. onnx파일로 test해보기
```
import onnxruntime as ort
import torch
import torchvision
from torchvision.transforms import Resize, Normalize
import numpy as np

class VideoMAE_ONNX_Pipeline:
    def __init__(self, onnx_path, num_frames, resize_to, mean, std, id2label):
        self.ort_sess = ort.InferenceSession(onnx_path)
        self.num_frames = num_frames
        self.resize_to = resize_to
        self.mean = mean
        self.std = std
        self.id2label = id2label

        self.resize = Resize(resize_to)
        self.normalize = Normalize(mean=mean, std=std)

    def __call__(self, video_path):
        # 1. 비디오 읽기 [T,H,W,C]
        video, _, _ = torchvision.io.read_video(video_path, pts_unit='sec')
        video = video.float()  # [T,H,W,C]

        # 2. 프레임 샘플링 (균등)
        T = video.shape[0]
        indices = torch.linspace(0, T - 1, steps=self.num_frames).long()
        video = video[indices]  # [num_frames,H,W,C]

        # 3. 채널 순서 변경 [T,H,W,C] -> [T,C,H,W]
        video = video.permute(0, 3, 1, 2)

        # 4. Resize 및 Normalize
        video = torch.stack([self.normalize(self.resize(frame)) for frame in video])  # [T,C,H,W]

        # 5. 배치 차원 추가 [B,T,C,H,W]
        video = video.unsqueeze(0)

        # 6. ONNX 입력 [B,T,C,H,W] -> numpy
        ort_inputs = {"pixel_values": video.numpy().astype(np.float32)}

        # 7. 추론
        logits = self.ort_sess.run(None, ort_inputs)[0]  # [B,num_classes]
        pred_id = np.argmax(logits, axis=1)[0]
        return self.id2label[pred_id]

# -------------------------------
# 설정
onnx_path = "./video_cls_model.onnx"
num_frames = 16
resize_to = (224, 224)
mean = [0.485, 0.456, 0.406]
std  = [0.229, 0.224, 0.225]
id2label = {
    0: "assult", 1: "datefight", 2: "robbery", 3: "burglary", 4: "trespass",
    5: "wander", 6: "vandalism", 7: "fight", 8: "dump", 9: "swoon", 10: "kidnap"
}

# pipeline 생성
pipeline = VideoMAE_ONNX_Pipeline(onnx_path, num_frames, resize_to, mean, std, id2label)

# 예측
video_file = './data_split/test/assult/10-1_cam01_assault03_place07_night_summer.mp4'
pred_label = pipeline(video_file)
print("예측 라벨:", pred_label)
```