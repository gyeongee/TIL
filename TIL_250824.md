
###Attention
문맥에 따라 다른 의미를 가지는 단어
주변 임베딩과의 관계 파악 ->관련 단어의 벡터 위치로 이동
한 임베딩에 담긴 정보를 다른 임베딩에 전달

query와 key의내적값이 양수인 관계 = 서로 관련이 많음 - attend to
Attetion head
query
key
key와 query의 내적 계산의 가중치의 합
Wq
Attention pattern(map), Attention map의 크기는 전체 문장의 길이 ** 2
Transformer를 훈련할 때 병렬적으로 훈련 - 뒷쪽의 단어가 앞의 단어에 영향을 주지 않음, 뒤쪽의 토큰에 영향을 주는 토큰들을 음의 무한대로 만듦 -> softmax 적용 시 0이되게
masking: 특정 위치의 토큰이 다른 위치의 토큰에게 영향을 주지않음
one headed Attention
self attetion
cross attetion:두 개의 다른 데이터를 처리할 때
Multi-headed attention: 여러 개의 병렬 작업을 실행, head가 96개
value up, value down
