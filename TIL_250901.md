### 제로샷(zero-shot)이미지 분류
- 특정 카테고리의 예시가 포함된 데이터를 학습되지 않은 모델을 사용해 이미지 분류를 수행하는 작업
- 추가 학습 데이터 없이 새로운 레이블이나 학습하지 못한 카테고리에 대해 모델을 일반화

###CLIP : 이미지와 텍스트를 같은 임베딩 공간에 매핑하여, 둘 사이의 관련도를 측정할 수 있게 만든 VLM(Vision Language Model)

### 직접 제로샷 이미지 분류 모델 추론 실행하기
```
from transformers import AutoProcessor,AutoModelForZeroShotImageClassifaction

# 이미지 전처리기
# 프로세서를 사용해 모델의 입력 준비
# 프로세서는 모델의 입력으로 사용하기 위해 이미지 크기를 변환하고 정규화하는 이미지 프로세서와 텍스트 입력을 처리하는 토크나이저로 구성
processor = AutoProcessor.from_pretrained("openai/clip-vit-large-patch14")
# 모델
model = AutoModelForZeroShotImageClassifaction.from_pretrained("openai/clip-vit-large-patch14")
```

```
# 전처리기
label = ["fox", "bear", "owl", "fish"]

inputs =processor(images=image,
                 text=label,
                 #pt : pytorch
                 return_tensors="pt",
                 padding=True)
```
```
import torch

# 추론
#no_grad(): 학습을 수행하지 않는 순방향 실행
with no_grad():
    ouputs = model(**inputs)

# 추론 결과 분석
# logits_per_image: 이미지와 텍스트 간의 유사도 점수(raw logits)
# logits_per_image의 shape:[batch_size, num_labels] 형태
# 첫 번째 샘플(배치에서 첫 이미지/텍스트 쌍의 결과)
logits = outputs.logits_per_image[0]
# 클래스 차원 기준 확률 계산.
probs = logits.softmax(dim=-1).numpy()

print(probs)
# [5.5710014e-05 4.3769804e-05 9.9989545e-01 4.9988234e-06]
```

### 매개변수
- input : 객체 하나 전달
- *input : 리스트/튜플 가변인자 -> 위치 인자 풀어서 전달
- **input : 딕셔너리 가변이자 -> 키워드 인자(key=value 형태) 풀어서 전달
Hugging Face에서 **inputs를 쓰는 이유는 입력값의 종류(텍스트, 이미지, 마스크 등)가 모델마다 달라질 수 있어서, 딕셔너리를 통째로 풀어서 전달하기 위해서예요.


```
# 모델에 입력을 전달하고, 결과를 후처리
for label, score in zip(label, probs) :
  print(f"{label} : {score}")
```