### Transformer
##### 
- Transformer: 트랜스포머 다음에 올 단어를 예측
Transformer 모델 = Attention + FeedFarword
마지막 층에서 다음에 올 토큰의 확률 분포를 계산하여 가장 높은 토큰을 출력
다음의 내용을 예측


- Embedding , Attention, MLPs, Unembedding
Tokens (이미지의 작은 조각)
벡터화
비슷한 것은 가까운 공간에 배치
Attention : 벡터의 각 열은 attention block을 통과하면서 서로 정보를 주고 받고 업데이트함 
현재 단어의 의미를 다른 단어와의 관계를 통해서 정리하고 파악 - 문맥 파악 
벡터 값들이 연산(Multilayer perceptron)을 거침
연산(Multilayer perceptron):벡터들끼리 정보를 주고 받지 않고 병렬 로 동일한 과정을 거침
각 벡터의 수치를 구하고 업데이트함
layer 사이에 정규화

Beginning: 네트워크의 입력단
Ending: 네트워크의 출력단

Backpropagation(역전파)
tensor,data(모델의 입력값)와 weights(모델 동작을 결정)

행렬과 벡터의 곱으로 이루어져 잇음

Embedding matrix : 각 단어에 대응되는 하나의 열을 가지고 있음
                           임베딩 행렬은 모델의 가중치 묶음
                           

단어를 벡터로 변환
단어 임베딩: 비슷한 뜻을 가지는 단어가 비슷한 위치에 있을 수 있게 학습 시킴
벡터의 내적: 두 텍스트가 얼마나 가까이에 있는지 계산하는 방법
벡터가 비슷한 방향 +
벡터가 반대방향 -
맥락을 이해할 수 있는 의미적인 값
주변 정보의 단어를 통해 예측
입력텍스트를 벡터 배열로 만드는 것은 Embeddig matrix에서 가져옴
마지막 행렬의 마지막 벡터의 어휘 토큰(Unembedding matrix)을 곱해서 리스트로 만들고 
softmax:확률 분포 값을 정규화(0과 1 사이로 만듦)

효율성으로 제일 마지막 벡터로 다음 단어를 예측

softmax with temperature - Temp
온도를 높이면 가능성이 낮은 단어를 찾을 확률이 높음
chatgpt는 Temp이 2이상으로 지정하지 않음

입력값 - logits 출력값 - Probabilities(확률)

