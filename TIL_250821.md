### 이미지 모델
#### CLIP 
 - CLIP 같은 모델은 **"이미지 ↔ 텍스트 매칭"**을 배움
 - 예시 상황

학습 데이터에:

"a cat sitting on the sofa" + 해당 이미지

"a living room with a sofa" + 해당 이미지

"a person sleeping on the sofa" + 해당 이미지
이런 식으로 다양한 "쇼파" 관련 텍스트–이미지 쌍이 들어있었다고 합시다.

학습 후

모델은 "sofa"라는 단어가 **시각적으로 어떤 패턴(넓적한 쿠션 모양, 팔걸이, 실내 배경 등)**과 연결되는지 배웁니다.

따라서 입력 텍스트로 "sofa"를 주면 → 이미지 안에서 "쇼파"가 들어간 사진들과 높은 유사도를 갖게 됩니다.

이때 "고양이가 있든 없든" 상관없이, 쇼파 자체의 특징으로 매칭이 이뤄집니다.

요약

"고양이" 텍스트 쿼리 → 고양이 특징 있는 이미지를 찾음 🐱

"쇼파" 텍스트 쿼리 → 쇼파 특징 있는 이미지를 찾음 🛋️

결국 CLIP은 텍스트 속 단어/문맥을 시각적 개념으로 일반화해두었기 때문에, 학습 때 봤던 다양한 쇼파 이미지 덕분에 "쇼파"를 잘 찾을 수 있습니다.

👉 즉, CLIP은 "라벨 분류기"가 아니라, 텍스트로 지정한 어떤 개념이든 이미지에서 매칭해내는 검색/매핑 모델이라고 보는 게 맞습니다.
#### ViT
