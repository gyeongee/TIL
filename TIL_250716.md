### Tensor에서 숫자 연산
- Tensor에 연산을 하면 기본적으로 autograd가 연산 그래프를 계속 쌓음
  - 기울기 자동계산 -> <font color=orange>텐서 값을 상수로 쓰고 싶을 때 detach() 사용</font>
  
| 변환                | 결과               | requires\_grad                |
| ----------------- | ---------------- | ----------------------------- |
| `float(tensor)`   | Python float 스칼라 | 없음 (gradient 추적 불가)           |
| `tensor.item()`   | Python float 스칼라 | 없음                            |
| `tensor.detach()` | Tensor           | requires\_grad=False          |
| `tensor.long()`   | Tensor           | **원래 상태 유지 (True/False 그대로)** |

### scalar 값으로 빼고 싶을때
#### detach(), item() 사용
```python

x = torch.tensor([3.5], requires_grad=True)
y = x.detach().long()
```
``` python
x = torch.tensor([3.5], requires_grad=True)
y = int(x.item())   # 3
type(y)             # <class 'int'>
```
1️⃣ train_loss는 requires_grad=True(데이터 학습 진행, 최적화 함수 구함)라서 <font color= orange>.numpy() 불가.</font>

> train_loss += float(loss) <font color= orange> 소수형으로 바꿔줘야함</font>
> 
2️⃣ valid_loss는 torch.no_grad() 안에서 계산돼(최적화 함수 구하지 않음) requires_grad=False. 

그래서 <font color= orange>.numpy() 바로 호출 가능.</font>

### 학습 모델 직접 구현
#### Sequential()
- 별도의 forward 메서드 없이 사용 가능
``` python
model2 = nn.Sequential(
    #입력층
    nn.Linear(X_train.size(-1),3),
    nn.ReLU(),

    nn.Linear(3,3),
    nn.ReLU(),

    nn.Linear(3,3),
    nn.ReLU(),
    # 출력층
    nn.Linear(3,y_train.size(-1)),
    nn.ReLU()
)
