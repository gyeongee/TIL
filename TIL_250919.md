##### 1-3. onnx íŒŒì¼ë¡œ ëª¨ë¸ í…ŒìŠ¤íŠ¸í•œ ê²°ê³¼ Gradioë¥¼ ì´ìš©í•˜ì—¬ ì›¹ì„œë¹„ìŠ¤ êµ¬í˜„
```
import gradio as gr
import torch
import torchvision
from torchvision.transforms import Resize, Normalize
import numpy as np
import onnxruntime as ort

# -------------------------------
# VideoMAE ONNX pipeline
class VideoMAE_ONNX_Pipeline:
    def __init__(self, onnx_path, num_frames, resize_to, mean, std, id2label):
        self.ort_sess = ort.InferenceSession(onnx_path)
        self.num_frames = num_frames
        self.resize_to = resize_to
        self.mean = mean
        self.std = std
        self.id2label = id2label

        self.resize = Resize(resize_to)
        self.normalize = Normalize(mean=mean, std=std)

    def __call__(self, video_path):
        # 1. ë¹„ë””ì˜¤ ì½ê¸° [T,H,W,C]
        video, _, _ = torchvision.io.read_video(video_path, pts_unit='sec')
        video = video.float()

        # 2. í”„ë ˆì„ ìƒ˜í”Œë§ (ê· ë“±)
        T = video.shape[0]
        indices = torch.linspace(0, T - 1, steps=self.num_frames).long()
        video = video[indices]

        # 3. ì±„ë„ ìˆœì„œ ë³€ê²½ [T,H,W,C] -> [T,C,H,W]
        video = video.permute(0, 3, 1, 2)

        # 4. Resize + Normalize
        video = torch.stack([self.normalize(self.resize(frame)) for frame in video])

        # 5. ë°°ì¹˜ ì°¨ì› ì¶”ê°€ [B,T,C,H,W]
        video = video.unsqueeze(0)

        # 6. ONNX ì…ë ¥
        ort_inputs = {"pixel_values": video.numpy().astype(np.float32)}

        # 7. ì¶”ë¡ 
        logits = self.ort_sess.run(None, ort_inputs)[0]
        pred_id = np.argmax(logits, axis=1)[0]

        return self.id2label[pred_id]

# -------------------------------
# ì„¤ì •
onnx_path = "./dmsrud/anomalous_behavior_video_cls_model/video_cls_model.onnx"
num_frames = 16
resize_to = (224, 224)
mean = [0.485, 0.456, 0.406]
std  = [0.229, 0.224, 0.225]
id2label = {
    0: "assult", 1: "datefight", 2: "robbery", 3: "burglary", 4: "trespass",
    5: "wander", 6: "vandalism", 7: "fight", 8: "dump", 9: "swoon", 10: "kidnap"
}

pipeline = VideoMAE_ONNX_Pipeline(onnx_path, num_frames, resize_to, mean, std, id2label)

# -------------------------------
# Gradioìš© í•¨ìˆ˜
def analyze_video(video_file):
    label = pipeline(video_file)
    message = f"{label} ê°’ì´ íƒì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ì¶œë™í•©ë‹ˆë‹¤ ğŸš¨"
    return message

# -------------------------------
# Gradio ì¸í„°í˜ì´ìŠ¤ (ë²„íŠ¼ ì¶”ê°€)
with gr.Blocks() as demo:
    gr.Markdown("## ğŸš“ ë¹„ë””ì˜¤ ì´ìƒí–‰ë™ íƒì§€ ì‹œìŠ¤í…œ")

    with gr.Row():
        video_input = gr.Video(label="ë¹„ë””ì˜¤ ì—…ë¡œë“œ")

    with gr.Row():
        btn = gr.Button("íƒì§€ ì‹œì‘")

    with gr.Row():
        message_output = gr.Textbox(label="íƒì§€ ê²°ê³¼")
        # video_output = gr.Video(label="ì…ë ¥ ë¹„ë””ì˜¤")

    # ë²„íŠ¼ í´ë¦­ ì‹œ ë¶„ì„ ì‹¤í–‰
    btn.click(fn=analyze_video, inputs=video_input, outputs=[message_output])#, video_output

demo.launch(share=True)
```